
*********************************************************************************************
*********************************************************************************************
*********************************************************************************************
* Hadoop HDFS installieren, konfigurieren, kontrollieren, testen, …
*********************************************************************************************
*********************************************************************************************
*********************************************************************************************


Mit SSH auf bd-1 einloggen (im Verzeichnis: /home/pkmlp/CAS_Big_Data/SwitchCluster/)
> ssh -i id_rsa_bd 86.119.31.109  -l ubuntu

User wechseln
> su - hduser (Passwort: hd)

Im Verzeichnis /opt die installierten Services ansehen
> cd /opt
> ls

Zurück ins Home-Verzeichnis
> cd

Den Name-Node formatieren
> hdfs namenode -format

Hadoop starten
> start-dfs.sh

Kontrolle ob Hadoop läuft
> jps

	Erwarteter Output (auf bd-1)
		- NameNode
		- SecondaryNameNode



Kontrollieren der Slaves (Worker-Nodes)

Mit SSH auf bd-3 einloggen (aus bd-1 heraus)
> ssh bd-3


Kontrolle ob Hadoop läuft
> jps

	Erwarteter Output (auf bd-3)
		- DataNode




Damit ist Hadoop HDFS gestartet :-) 
Nun starten wir mit den ersten Schritten mit HDFS


Auf hdfs ein Verzeichnis anlegen:

> hdfs dfs -mkdir /user
> hdfs dfs -ls /
> hdfs dfs -mkdir /user/hduser


Ein File vom lokalen (bd-1) Filesystem auf hdfs kopieren:

Auf bd-1 ins Home-Verzeichnis gehen
> cd ~

Das File testme.txt anschauen
> more testme.txt

Das File testme.txt auf hadoop laden
> hdfs dfs -put ~/testme.txt /user/hduser
> hdfs dfs -ls /



Hdfs via Browser einsehen

Öffnen Sie einen Browser auf Ihrem Laptop
> http://86.119.31.109:50070


Im Browser im Menü “Utilities”, Untermenu “Browse the filesystem”
/ eingeben und Go! Klicken um im hdfs-Filesystem zu browsen.




*********************************************************************************************
*********************************************************************************************
*********************************************************************************************
* YARN installieren, konfigurieren, kontrollieren, testen, …
*********************************************************************************************
*********************************************************************************************
*********************************************************************************************

Auf bd-1 mit user hduser (weiter) arbeiten

	Mit SSH auf bd-1 einloggen (im Verzeichnis: /home/pkmlp/CAS_Big_Data/SwitchCluster/)
	> ssh -i id_rsa_bd 86.119.31.109  -l ubuntu
	
	User wechseln
	> su - hduser  (Passwort: hd)


YARN starten
> start-yarn.sh

Kontrolle ob Yarn läuft
> jps


	Erwarteter Output - zusätzlich neu:
		> ResourceManager
		

Kontrolliere auf den Worker-Nodes (z.B. bd-3)

	Erwartet wird zusätzlich neu:
		> NodeManager



Yarn-Prozesse einsehen - Web Interface zum Resource Manager
Öffnen Sie auf Ihrem Laptop einen Browser, resp einen neuen Tab im Browser
> http://86.119.31.109:8088





*********************************************************************************************
*********************************************************************************************
*********************************************************************************************
* HIVE und Zeppelin
*********************************************************************************************
*********************************************************************************************
*********************************************************************************************
*
* SwitchCluster aus WebBrowser starten
*
https://engines.switch.ch/horizon/quickstart/



*
* aus Verzeichnis /home/pkmlp/CAS_Big_Data/SwitchCluster in SwitchCluster einloggen 
* und User auf hduser wechseln
*
ssh -i id_rsa_bd 86.119.31.109  -l ubuntu
su - hduser



*
* auf bd-1 als hduser
*
start-dfs.sh
start-yarn.sh
sudo service mysql start
hive --service metastore &
hiveserver2 &

web ui Öffnen: http://86.119.31.109:10002/ 



*
* auf bd-2 als hduser
*
zeppelin-daemon.sh start

web ui Öffnen: http://86.119.31.103:8080/  (admin/pkmlp)





*********************************************************************************************
*********************************************************************************************
*********************************************************************************************
* Spark (PVA3)
*********************************************************************************************
*********************************************************************************************
*********************************************************************************************
*
* SwitchCluster aus WebBrowser starten
*
https://engines.switch.ch/horizon/quickstart/



*
* Spark Cluster auf SwitchCluster:
*   - bd-1:  Cluster-Manager (aka Driver)
*   - bd-3:  Worker
*   - bd-4:  Worker
*   - bd-5:  Worker
*

*
* aus Verzeichnis /home/pkmlp/CAS_Big_Data/SwitchCluster in SwitchCluster einloggen 
* und User auf hduser wechseln
*
ssh -i id_rsa_bd 86.119.31.109  -l ubuntu
su - hduser

*
* auf bd-1 als hduser
*
start-dfs.sh
spark-start-all.sh
sudo service mysql start
hive --service metastore &
hiveserver2 &

*
* web-ui Spark: http://86.119.31.109:8080/ 
*


*
* auf bd-2 als hduser
*
zeppelin-daemon.sh start

web ui Öffnen: http://86.119.31.103:8080/  (admin/pkmlp)






*********************************************************************************************
*********************************************************************************************
*********************************************************************************************
* Streaming (PVA3)
*********************************************************************************************
*********************************************************************************************
*********************************************************************************************
*
*
* aus Verzeichnis /home/pkmlp/CAS_Big_Data/SwitchCluster in SwitchCluster einloggen 
* und User auf hduser wechseln
*
ssh -i id_rsa_bd 86.119.31.109  -l ubuntu
su - hduser

*
* auf bd-1 als hduser
*
start-dfs.sh
spark-start-all.sh
sudo service mysql start
hive --service metastore &
hiveserver2 &

*
* web-ui Spark: http://86.119.31.109:8080/ 
*


*
* auf bd-2 als hduser
*
zeppelin-daemon.sh start

web ui Öffnen: http://86.119.31.103:8080/  (admin/pkmlp)






